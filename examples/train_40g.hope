## 表示作业的基本信息，自动填充，请勿修改
[base]
type = ml-vision

[resource]
usergroup = hadoop-mtcv
# queue = root.zw05_training_cluster.hadoop-vision.mmt
queue = root.zw05_training_cluster.hadoop-vision.elastic_job
# queue = root.zw05_training_cluster.hadoop-vision.job

## 数据集信息（支持配置多个，位置需一一对应，','分隔）
[dataset]
dataset_name =
dataset_type =
dataset_path =

## 作业串联相关配置
[job_track]
## 上游任务id（支持配置多个，','分隔）
upstream_jobid =
## 输入目录配置（支持配置多个，','分隔）
input_dir =
## 输出目录配置（支持配置多个，','分隔）
output_dir =
## 日志输出目录（支持配置多个，','分隔）
log_dir =
demand_id = 4016


[roles]
workers = 1
# worker.memory = 450000
# worker.vcore = 60
# worker.gcores40g = 4
worker.memory = 900000
worker.vcore = 120
worker.gcores40g = 8

## worker启动后执行的脚本，一般为训练作业的执行命令
worker.script = bash examples/tvg/run_qwen2_5_vl_3b_tvg_correct.sh 8


## worker端python脚本的输入参数
## 可以设置args.batch_size = 32，则会向worker.script追加参数--batch_size=32
[user_args]
# args.script = ostrack
# args.config = vitb_256_tbsi_care_loc2468_32x4_4e4_lasher_15_0127
# args.save_dir = /mnt/dolphinfs/hdd_pool/docker/user/hadoop-mtcv/huangshaofei/debug
# args.mode = single

[am]
afo.app.am.resource.mb = 4096

[tensorboard]
with.tensor.board = false

## docker环境配置
[docker]
# afo.docker.image.name = registry-offlinebiz.sankuai.com/custom_prod/com.sankuai.phxmlp.mtjupyter.singleuser/mtcv_training_pytorch2.1.0_cuda12.4.0_python3.10_llamafac_38fc6989
# afo.docker.image.name = registryonline-hulk.sankuai.com/custom_prod/com.sankuai.data.hadoop.gpu/data-cuda12.1.1_v1-6aef3f99 
afo.docker.image.name = registryonline-hulk.sankuai.com/custom_prod/com.sankuai.data.hadoop.gpu/data-hadoop-platcv_py39-cuda12.1-pt2.1.2-trt8.6.1.6-triton23.02-vllm0.3-2def3973
# afo.docker.image.name = registryonline-hulk.sankuai.com/custom_prod/com.sankuai.data.hadoop.gpu/data-system_base_c7_cuda11.8_cudnn8.8.1.3_v1-47834059
## 是否使用预拉取
[data]
afo.data.prefetch=false

## 是否支持容错
[failover]
afo.app.support.engine.failover=true

## conda环境上传
[conda]
# afo.conda.env.name = llavaov
# afo.conda.env.path = viewfs://hadoop-meituan/zw03mlnn01/user/conda/huangshaofei/default/lane3d.tar.gz
# afo.conda.store.type = hdfs

## 可合并xml配置文件，输入当前目录的xml文件名
# [config]
# config.file =

## 多机情况下可以配置不同的分布式模式，默认取值为tensorflow，代表tensorflow/ps架构。其他取值有mpi，代表mpi/horovod架构；pytorch，代表pytorch/ddp架构。
# [distribute]
# distribute.mode =

[others]
## pytorch dataloader可能会用到共享内存，配置需要的共享内存（单位为B）
afo.app.env.YARN_CONTAINER_RUNTIME_DOCKER_SHM_SIZE_BYTES=5242880000000
## 作业结束后，会通过大象通知用户
afo.xm.notice.receivers.account=weiziyu@meituan.com
## 若配置true，则会安装.hope文件同路径下requirements.txt中配置的依赖
with_requirements = false
## 作业排队时间上限，单位秒
afo.app.yarn.allocate.timeout.seconds = 84600
## 加快作业的提交速度
afo.use.acceleration.submission=true
# afo.dolphinfs.otherusers = hadoop-vision-common

afo.docker.rw.volume.paths = /mnt/beegfs/hdd_pool/docker/user/hadoop-vision-common:/mnt/dolphinfs/hdd_pool/docker/user/hadoop-vision-common,/mnt/beegfs/hdd_pool/docker/user/hadoop-imagen:/mnt/dolphinfs/hdd_pool/docker/user/hadoop-imagen,/mnt/beegfs/hdd_pool/docker/user/hadoop-vision-data:/mnt/dolphinfs/hdd_pool/docker/user/hadoop-vision-data,/mnt/beegfs/ssd_pool/docker/user/hadoop-platcv:/mnt/dolphinfs/ssd_pool/docker/user/hadoop-platcv

afo.use.hdfs.fuse = true
afo.use.hdfs.fuse.subpath = user/hadoop-aipnlp:/mnt/hdfs/user/hadoop-aipnlp